%%This is a very basic article template.
%%There is just one section and two subsections.
%\documentclass{article}


\documentclass[a4paper,twoside]{article}

\usepackage{icgst}
\usepackage{algorithm}
\usepackage{algorithmic}

%\documentclass[preprint,12pt]{elsarticle}
%\journal{Jornal Name in the document\ldots\ldots\ldots.}
\begin{document}

\input{files/head}
\input{files/abstract}
%\input{files/head2}




\section{Introduction}

Face recognition have become one of the most extensively studied research topics that spans different disciplines such as pattern recognition, image processing and artificial intelligence. This is due to its importance to various application like identity authentication, surveillance and intelligents vision. One of the ost used method is egin faces which is used to extract relevant facial information from faces to used as features to identificy faces\cite{}.

There is various statistical features used to detect like DCT, wavelet transform and other information Using data clustering as classification was used. The most used feature was used is the eigen faces which transform the face image into feature space that represents important facial information.


The clustring algorithm is widely used to  cluster information in data and text mining and many other application \cite{}. The use of clustring algorithm in classificatin was also reproted in \cite{} where kmeans algorithms is used to lablel on the input samples to one of the trained clusters. 
The use of Particle swarm optimization (PSO) in data clustring was introduced by Merwe and Engelbrechet \cite{psoclustering}. They presented a system that was test on three different dataset and achieved better clustring than other clustring algorithms. Several enhancment has been added to the PSO algorithms to improve its efficiency and quality of the global solution. Several other search method like Nelder-Meed \cite{}  were also  integrated to the PSO algorithm. The use of Nelder-Meed search in the PSO proofed to enchance the quality of the global optimium solution with little computational cost \cite{}. 


In this paper we introduce a novel face recognition algorithm that uses a Particle swarm optimization (PSO) to create clusters of faces in the database. The Nelder Meed enchancment is integarted into the PSO algorithm to create clusters where Each cluster represents a single person in the database.  The first step of the system is preprocessing the database to detect and normalize the face inside the image. Secondly, feature vectors are extracted by computing the  eigenfaces for every person in the database then extracting the wieght vector for each sample in the database. After that the training set is used to create clusters for the PSO clustering algorithm. The global solution should represents a defined center for all clusters where each cluster represent a single person. The problem of recognition of test sample is converted into identifying the cluster that the input sample belogs to. % step  %and the eigen vector are extracted. The  and 

%Face recognition (FR) has emerged as one of the most extensively studied
%research topics that spans multiple disciplines such as pattern recognition,
%signal processing  and computer vision. This is due to its numerous  important
%applications in identity authentication, security  access control, intelligent
%human-computer interaction,  and automatic indexing of image and video
%databases.  Many approaches to face recognitions have been  developed; an
%excellent survey paper on the different face recognition techniques can be
%found in [1].



% Face recognition systems have been grabbing high attention from commercial market point of view as well as pattern recognition field. Face recognition has received substantial biometrics, pattern recognition communities[11][13]. The face
% field recognition systems can extract
% attention from researches in and computer
% Eigenfaces are mostly used to: a. Extract the relevant facial information, which may or may not
% be directly related to human intuition of face features such as the eyes, nose, and lips.
%One way to do so is to capture the statistical variation between face images.
%b. Represent face images efficiently. To reduce the computation
% and space complexity, each face image can be represented using a small number of dimensions The eigenfaces may be considered as a set of features which characterize the global variation among face images. Then each face image is approximated using a subset of the eigenfaces, those associated with the largest eigenvalues. These features account for the most variance in the training set. In the language of information theory, we want to extract the relevant information in face image, encode it as efficiently as possible, and compare one face with a database of models encoded similarly. A simple approach to extracting the information contained in an image is to somehow capture
% the variations in a collection of face images,
% independently encode and compare individual face images. Mathematically, it is simply finding the principal components of the
% vision the
% features of face and compare this with the existing database. The faces considered here for comparison are still faces. Machine recognition of faces from still and video images is emerging as
% an active research area[11]. The present paper is formulated based on still or video images captured either by a digital camera or by a web cam. The face recognition system detects only the faces from the image scene, extracts the descriptive features. It later compares with the database of faces, which is collection of faces in different poses. The
 

The remaining of the paper is organized as follows. Firstly, section
\ref{sec:section2} gives the basic background infromation on  Eigenfaces,
Particle swarm optimization,and Nelder Meed modification Section
\ref{sec:proposed} describes in details our proposed system. Section \ref{sec:Results} presents the experiments undertaken to develop the system and also some experiments to evaluate the recognition performance of the proposed system. Finally, in section \ref{sec:Conclusion} we present conclusion and future works.



\section{Background Information}
\label{sec:section2}
In this section we will describe the basic information that is required to
explain our proposed system.  We will start by explaining how to compute Eigen
faces then proceed to explain the general Particle swarm algorithm.

\subsection{Eigen Faces}
\label{facereg}
A set of eigenfaces \cite{facesite} can be generated by performing a mathematical process called principal component analysis (PCA) on a large set of images depicting different human faces. Informally, eigenfaces can be considered a set of "standardized face ingredients", derived from statistical analysis of many pictures of faces. Any human face can be considered to be a combination of these standard faces.


 The aim is to represent a face as a linear combination of a set of basis images called eigenfaces. That is :
\begin{equation}
% MathType!MTEF!2!1!+-
% feaafiart1ev1aaatCvAUfeBSjuyZL2yd9gzLbvyNv2CaerbuLwBLn
% hiov2DGi1BTfMBaeXatLxBI9gBaerbd9wDYLwzYbItLDharqqtubsr
% 4rNCHbGeaGqiVu0Je9sqqrpepC0xbbL8F4rqqrFfpeea0xe9Lq-Jc9
% vqaqpepm0xbba9pwe9Q8fs0-yqaqpepae9pg0FirpepeKkFr0xfr-x
% fr-xb9adbaqaaeGaciGaaiaabeqaamaabaabaaGcbaGaeuOPdy0aaS
% baaSqaaiaadMgaaeqaaOGaeyypa0ZaaabCaeaacqaHjpWDdaWgaaWc
% baGaamyAaaqabaGccaWG1bWaaSbaaSqaaiaadQgaaeqaaaqaaiaadQ
% gacqGH9aqpcaaIXaaabaGaam4saaqdcqGHris5aaaa!444E!
\Phi _i  = \sum\limits_{j = 1}^K {\omega _i u_j }
\end{equation}

Where $\Phi_i$ represents the $i^{th}$ face with the mean subtracted from it, $\omega _i$ represent weights and  $u_j$ the eigenvectors.
This can be represented aptly in a figure as:

\begin{figure}[h]
 \begin{center}
 \centering
\includegraphics[scale=0.5]{eigenfacesrec.jpg} \caption{Face reconstruction from eigen faces. }
 \end{center}\end{figure}


\subsubsection{Assumptions:}

\begin{enumerate}

\item There are $M$ images in the training set.

\item There are $K$ most significant Eigenfaces using which we can satisfactorily approximate a face. Needless to say $K <M$.

\item All images are $NxN$ matrices, which can be represented as dimensional vectors. The same logic would apply to images that are not of equal length and breadths. To take an example: An image of size 112 x 112 can be represented as a vector of dimension 12544 or simply as a point in a 12544 dimensional space.

\end{enumerate}

\subsubsection{Algorithm for Finding Eigenfaces}

\begin{enumerate}
\item Obtain $M$ training images , $I_1, I_2 \dots I_m$  , centered faces images.

%\includegraphics{training-images.jpg}
\begin{figure}[h]
 \begin{center}
 \centering
\includegraphics[scale=0.5]{training-images.jpg} \caption{Sample Faces. }
 \end{center}\end{figure}
\item Represent each image $I_i$ as a vector $\Gamma_i$.
\begin{equation}
\label{arrequation}
I_i  = \left[ {\begin{array}{*{20}c} {a_{11} } & {a_{12} } &\ldots& {a_{1N} }  \\
   {a_{21} } & {a_{22} } &  \ldots   & {a_{2N} }  \\
    \vdots  &  \vdots  &  \ddots   & {}  \\
   {a_{N1} } & {a_{N2} }  & {} & {a_{NN} }  \\
\end{array}} \right]_{NxN} \Rightarrow\left[
{\begin{array}{*{20}c}
   {a_{11} }  \\
   {\vdots}  \\
    {a_{1N} }  \\
     {a_{21}}  \\
        {\vdots}  \\
   {a_{2N} }  \\
   {\vdots}  \\
   {a_{NN} }  \\
\end{array}} \right]_{N^2 x1} = \Gamma _i
\end{equation}
\item Find the average or the mean face vector  $\Psi$

\begin{equation}
\Psi  = \frac{1}{M}\sum\limits_{i = 1}^M {\Gamma _i }
\end{equation}
\item  Subtract the mean face $\Psi$ from each face vector $\Gamma_i$ to get a set of vectors $\Phi_i$. The purpose of subtracting the mean image from each image vector is to be left with only the distinguishing features from each face and (removing)  in a way information that is common.
\begin{equation}
\Phi _i  = \Gamma_i  - \Psi
\end{equation}
\item  Find the Covariance matrix $C$:
\begin{equation}
C = AA^T
\end{equation} , where $A = \left[ {\begin{array}{*{20}c}
   {\Phi _1 } & {\Phi_2} & {\dots} & {\Phi _M }  \\
\end{array}} \right]
$
Note that the Covariance matrix has simply been made by putting one modified image vector obtained in  one column each.

Also note that $C$is a $N^2xN^2$ matrix and $A$ is a $NxM$ matrix.


\item  Instead of the Matrix $AA^T$  consider the matrix $A^TA$ .


\item Find the $M$ best  Eigenvectors of $C=A^TA$. using the relation of $u_i=Av_i$  Also keep in mind that $\left\| {u_i} \right\| = 1.$ where  $u_i$ is the eigen vector of $C$ and $v_i$ is the eigen value of the matrix $C$.



\item Select the best  Eigenvectors, the selection of these Eigenvectors is done heuristically.






\item Finding Weights:\\

The Eigenvectors found at the end of the previous section,  when converted to a matrix in a process that is reverse to that in STEP \ref{facereg}, have a face like appearance. Since these are Eigenvectors and have a face like appearance, they are called Eigenfaces. Sometimes, they are also called as Ghost Images because of their weird appearance.

Now each face in the training set (minus the mean),$\Phi_i$  can be represented as a linear combination of these Eigenvectors   $u_j$:
  \begin{equation}
\Phi_i  = \sum\nolimits_{j = 1}^k {\omega_j u_j}
  \end{equation}
, where  $u_j$?s are Eigenfaces and $\omega_j$ are the weights and can be calculated as :
\begin{equation}
\label{weighteq}
\omega _j  = u_j^T \Phi _i
\end{equation}
Each normalized training image is represented in this basis as a vector $\Omega$.
\end{enumerate}

\subsection{Particle Swarm Optimization}
%\section{Discrete Particle Swarm Optimization}
\label{sec:ParticleSwarmAlgorithm}
 The main idea of \textit{Particle Swarm Algorithm (PSO)} is to represent each solution with a $N$ dimension particle from the solution space \cite{PSOFirst}. Each particle moves with a direction and velocity $v_{ij}$ based on equations \ref{eq:Swarm1} \& \ref{eq:Swarm}.

\begin{equation}
%\[
p_{ij}=p_{ij}+v_{ij},
%\
\label{eq:Swarm1}
\end{equation}

where $p_{ij}$ represent the $j$th dimension in the $i$th particle and $v_{ij}$ is the velocity of the $j$th dimension in the $i$th particle.
 %Equation [\ref{eq:Swarm}] shows how velocity and direction of each particle are computed
 \begin{equation}
v_{ij}  = v_{ij} \omega + c_1 r_1 (lbest_{ij}  - p_{ij} ) + c_2 r_2 (gbest_{ij}  - p_{ij} ),
\label{eq:Swarm}
\end{equation}
 where $\omega$ is the inertia weight parameter which controls the tradeoff between exploration and exploitation,  $lbest_{ij}$ is the local best particle, $gbest_{ij}$ is the global best particle, $r_1$ \& $r_2$ are random variables and $c_1$ \& $c_2$ are the swarm acceleration parameters.

 After each iteration the global best $g_{best}$ particle and the agent local best particle $l_{best}$are evaluated based on the maximum fitness functions of all particles in the solution space. The solution is found after achieving a specific number of iteration or after an error threshold is achieved.
Equation \ref{eq:descrite} is used to change the general swarm algorithm into binary particle (\textit{Discrete Particle Swarm Algorithm DPSO}). The \textit{DPSO} handles particle values of either $0$ or $1$ \cite{PSODisceret}.
\begin{equation}
   P(i)\Leftarrow
\left \{
\begin{array}{c}
1 \quad \quad if\quad r_{3}>p_{i}  \\

0 \quad \quad if\quad r_{3}<p_{i},
\label{eq:descrite}
\end{array}\right.
\end{equation}
 where $p_{ij}$ is the numerical values of the particle and $r_{3}$ is a random variable.


 \subsection{Nelder-Mead Enhancment}

The Nelder-Mead simplex algorithm is a deriavative free search method that finds the local minimum of f
A simplex method for finding a local minimum of a function of several variables has
been devised by Nelder and Mead. For two variables, a simplex is a triangle, and
the method is a pattern search that compares function values at the three vertices of a
triangle. The worst vertex, where f (x, y) is largest, is rejected and replaced with a new
vertex. A new triangle is formed and the search is continued. The process generates
a sequence of triangles (which might have different shapes), for which the function
values at the vertices get smaller and smaller. The size of the triangles is reduced and
the coordinates of the minimum point are found.


The Nelder-Mead simplex algorithm is a derivative-free local search technique
that is capable of moving a cluster of solutions in the gradient direction and which, as per current research, can be very effectively combined with GA and PSO approaches. These hybrid evolutionary algorithms have been shown to be very successful in continuous optimization problems.
The Nelder-Mead smplex method makes use of a construct called a simplex (see
Figure 1.). When the search space is n-dimensional, the simplex consists of n+1 solutions, s,, i = {1, 2,\dots  , n+1}, that are usually closely spaced. As shown in the top left of Figure 1., in a two-dimensional search plane, a simplex is a triangle. The fitness of each solution is considered in each step of the Nelder-Mead method, and the worst solution w is identified. The centroid, c, of the remaining n points is computed and the reflection of walong it determined. This reflection yields a new solution r that replaces w, in the next step, as shown in the top right of Figure 1. If the solution r produced by this reflection has a higher fitness than any other solution in the simplex, the simplex is further expanded along the direction of r, as shown in the middle left of the figure. On the other hand, if r has a low fitness compared to the others, the simplex is contracted. Contraction can be either outward or inward depending upon whether r is better or worse than w. The contraction operations are shown in the middle right and bottom left of the figure. If neither contraction improves the worst solution in the simplex, the best point in the simplex is computed, and a collapse is then carried out, and all the points of the simplex are moved a little closer towards the best one, as shown in the bottom right of the same figure.
\section{Proposed System}
\label{sec:proposed}

Every recognition system consists of trainning and testing modules. Figure
\ref{Blockchart} shows the flow chart for the training and testing modules.
The training is divided into three steps 1) preprocessing, 2) feature extraction
and then 3) clustring using a hybird Particle Swarm Optimization. The follwoing
few section will discribe in details each step.  %step is the system is divided
into
%two major steps 1) preprocessing 2) training




\begin{figure}
 \begin{center}
 \centering
%\includegraphics[scale=0.5]{blockchart.jpg}
 \subfigure['Training']{\label{fig:block1}\includegraphics[width=0.45\textwidth]{block1.jpg}}
  \subfigure['Testing']{\label{fig:block2}\includegraphics[width=0.45\textwidth]{block2.jpg}}

\caption{ The flow Chart}
\label{Blockchart}
 \end{center}\end{figure}
\subsection{Preprocessing}
The preprocessing step function is to detect the face from the input images
then normalize and the face to the center of each  image.  Every face
image in the database is  then scalled to the constant size of $h \times w$
\footnote{ note that  $h$ height and $w$ wdith are emprically choose to be 60
adn 80 respecitivily.}. The next step is computing the eigenfaces for each
person in the dataset as explianed in section \ref{facereg}.


\subsection{Feature Extraction}

After the computing the eigenfaces of every person on the database the system
extracts eigen weight vector for each image. Equation \ref{} shows how the
weight vector is computed from the eigen faces. These weights are used as
features for the clustering algorithm.


\subsection{PSO clustering}
The clustering algorithm is used to train the system to identify the faces in
the dataset. The clustering algorithm used is based on Particle algorith based
on the Merwe and Engelbrechet \cite{psoclustering} Method. Each particle
$X_i$ is constructed such that that
\begin{equation}
X_i=\left( m_{i1}, m_{i2}, m_{i3},\dots, m_{iN_c}\right)
\end{equation}
where $N_c$ is the number of clusters to be formed and $m_{ij}$ corresponds to the $j^{th}$ centroid of the  $i^{th}$ particle, the centroid of the cluster $C_{ij}$. Thus single particle represents a candidate solution to a given clustering algorithm. Each particle is evaluated using the following equation:
% \begin{equation}
% F_c=\frac{\sum^{N_c}_{j=1}\right[\sum_{\forall Z_j \epsilon C_{ij}}\frac{ d\right(Z_p,m_{ij}\left)}{|C_{ij}|}  \left]}{N_c}
% \end{equation}
\begin{equation}
F_c  = \frac{{\sum\nolimits_{j = 1}^{N_c } {\left[ {\sum\limits_{\forall z_j  \in C_{ij} } {\frac{{d(z_p ,m_{ij} )}}{{\left| {C_{ij} } \right|}}} } \right]} }}{{N_c }}
\end{equation}
where $F_c$ is the fitness function, $N_c$ is the number of clusters in the particle, $Z_p$ denotes the $p^{th}$ data vector $\left| {C_{ij} } \right|$ is the number of data vectors belonging to the cluster $C_{ij}$ and $d$ is the Euclidian distance between $Z_p$ and $m_{ij}$.


The clustering algorithm prposed can be as follows:
% \begin{algorithm}
%  \caption{ Discrete Particle Swarm Algorithm }
%  \label{DPSOalg}
\begin{algorithmic}
%\begin{verbatim}
\STATE  Generate $N$ particles with $N_c$ clusters  ($P_i,$ $\dots,$ $P_{N_c}$)
\STATE Initialize the clusters centroids for each particle randomly
\FOR{i=0 \TO $t_{max}$}
  \FOR{ each particle  $p_i$}
    \FOR{ each data vector  $zp$}
        \STATE   Calculate distance $d(zp,m_{ij})$ from cluster centroid.
        \STATE   Assign $zp$ to cluster $C_{ij}$ such that
        \STATE   $d(zp,m_{ij})=min \forall k=1,\dots,N_c {d(zp,m_{ik})}$
    \ENDFOR
    \STATE  Compute the Fitness function for the particle.
     \STATE  Update the global best and local best
     \STATE  Move all particle using PSO equations.
     \ENDFOR
  \ENDFOR
 \RETURN global best
%\end{verbatim}

 \end{algorithmic}

 Figure shows the \ref{fig:pso1} the block diagram for the PSO clustering algorithm. 



\begin{figure}
 \begin{center}
 \centering
%\includegraphics[scale=0.5]{blockchart.jpg}
 \subfigure['PSO
 clustering ']{\label{fig:pso1}\includegraphics[width=0.25\textwidth]{pso1.jpg}}
 \subfigure['Nelder
 meed a']{\label{fig:pso2}\includegraphics[width=0.25\textwidth]{pso2.jpg}}
  \subfigure['Nelder
  meed b']{\label{fig:pso3}\includegraphics[width=0.25\textwidth]{pso3.jpg}}
\caption{ Particle Clustering Alogirhtm}
\label{fig:PSOswarm}
 \end{center}\end{figure}
Both previous algorithms  were implemented and tested for correctness. Also the k-means algorithm was used to compare the result of the PSO clustering algorithm. The code was implemented using Matlab and using a Database as train and test set. The Databases contains 50 persons and 4 images per person. Currenlty all the images are used for training and the same set is used for testing.


The PSO algorithm is used to clusters each face weights computed using equation \ref{weighteq}. The algorithms first compute the eigen vectors for all 50 faces as explained in section \ref{facereg}. The computed $u_j$ is used to compute the weights for each image in the database is computed which generate a data matrix of 150 X 50 ( number of image X weight vector (no of eigenfaces)). The clustering algorithm is used to get center weigh vector for each person. I choose the number of clusters same as number of person for easier recognition step.


The recognition step will be easier as to compute the weigh vector of the input image and then compute the distance from each cluster centroid. The cluster with the min distance is the cluster of the face.

\section{Results}
\label{sec:Results}
Two different databases \cite{JainMukherjeeIndianFaceDB}was processed, the first
contains 60 indian faces with 11 different samples with different poses and experisions. The second database is 120 eroupian faces each has 20 samples. The database is processed to the input format for the particle algorithm. The data is then divided into training and testset. The percentage can be changed and changed to cross validate the results and experiments.

\subsection{Experiments Finding PSO paramters}
\begin{table}
	\centering
		\caption{PSO Result }
	\label{tab:PSO1}
	%\scalebox{0.99}{
		\begin{tabular}{|l|c|c|c|c|c|}
		 \hline
C1 &	1	&1.2&	1.6&	1.8 &	2	\\ \hline
	&70.893	&70.893	&70.605	&70.029	&69.741 \\  \hline
C2	&	1	&1.2&	1.6&	1.8 &	2	\\ \hline
	&70.89	&70.61	&70.89	&69.45	&69.45 \\ \hline
W	&0.5&	1	&1.5	&2&	4	\\ \hline
	&69.74	&70.89&	70.89	&70.32&	70.89	\\ \hline
\end{tabular}
		%}
\end{table}


\subsection {Comparing With other systems}

\subsection{ Comparing Nelder enhancement}

\section{Conclusion}
\label{sec:Conclusion}
\section{future work}
% \subsection{Subtitle}
%
% Plain text.
%
% \subsection{Another subtitle}
%
% More plain text.
\bibliographystyle{model1a-num-names}
\bibliography{mybibfile,library}

\end{document}
